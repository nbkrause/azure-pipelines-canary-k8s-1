# specific branch build with batching
trigger:
  # If you set "batch: true", when a pipeline is running, the system waits until the run is completed,
  # then starts another run with all changes that have not yet been built. This avoids simultaneous runs
  # of the cannary that could mess up the traffic.
  batch: true
  branches:
    include:
    - master

pool:
  vmImage: Ubuntu-16.04

resources:
- repo: self

variables:
  namespace: default
  # tags for images: the tag for the image in the base Deployment, and the tag in the new build
  baseTag: '$(Build.BuildId)'
  buildTag: '$(Build.BuildId)'
  # replace with name of your Docker registry service connection
  # we assume the registry will have the same name (ie, $(containerRegistry).azurecr.io)
  containerRegistry: azurepipelinescanaryk8s
  environment: k8sEnvironment.default # replace with the of your Kubernetes service connection and namespace
  canaryEnvironment: canaryEnvironment.default
  imageName: azure-pipelines-canary-k8s

stages:
- stage: Build
  displayName: Build stage
  jobs:
    # build the image for `./app` and push it to $(containerRegistry)
  - job: Build
    displayName: Build
    steps:
    - task: Docker@2
      displayName: Build and push image
      inputs:
        containerRegistry: $(containerRegistry)
        repository: $(imageName)
        command: buildAndPush
        Dockerfile: app/Dockerfile
        tags: $(buildTag)
    - upload: manifests
      artifact: manifests
    - upload: misc
      artifact: misc

- stage: Deploy
  displayName: Deploy stage
  dependsOn: Build

  jobs:
  - deployment: Deploy_Static_Manifests
    displayName: Deploy Static Manifests
    environment: $(environment)
    strategy:
      runOnce:
        deploy:
          steps:
          - checkout: self
          - task: KubernetesManifest@0
            displayName: Create Secret
            inputs:
              action: createSecret
              namespace: $(namespace)
              secretType: dockerRegistry
              secretName: $(imageName)
              dockerRegistryEndpoint: $(containerRegistry)

          # Other deployments in the Kubernetes cluster
          - task: KubernetesManifest@0
            displayName: Deploy Fortio and ServiceMonitor
            inputs:
              action: deploy
              manifests: $(Build.SourcesDirectory)/misc/*

  - deployment: Deploy_Base
    dependsOn: Deploy_Static_Manifests
    displayName: Deploy Base
    environment: $(environment)
    strategy:
      runOnce:
        deploy:
          steps:
          # Base deployment of the application
          # This should be a stable deployment.
          - checkout: self
          - task: KubernetesManifest@0
            displayName: Create base deployment
            inputs:
              action: deploy
              namespace: $(namespace)
              manifests: $(Build.SourcesDirectory)/manifests/*.yml
              containers: $(containerRegistry).azurecr.io/$(imageName):$(baseTag)
              imagePullSecrets: $(imageName)

  - deployment: Deploy_Canary
    dependsOn: Deploy_Base
    displayName: Deploy Canary
    environment: $(canaryEnvironment)
    strategy:
      canary:
        increments: [25, 50]

        deploy:
          steps:
          # create a clone of the Base manifests we already applied, but "canarized":
          # we will generate a Service/Deployment/Mapping for the Canary, with the built image
          - checkout: self
          - task: UsePythonVersion@0
            displayName: Set Python 3
            inputs:
              versionSpec: '3.x'
          - script: ./deploy/canarize.py --debug -m -l canary=true,build=$(Build.BuildNumber) --image $(containerRegistry).azurecr.io/$(imageName):$(buildTag) -o $(Pipeline.Workspace)/canary.yml -w $(strategy.increment) --image $(containerRegistry).azurecr.io/$(imageName):$(buildTag) $(Build.SourcesDirectory)/manifests/*.yml
            displayName: Generate canary for $(strategy.increment)% traffic

          # apply the canary Service/Deployment/Mapping
          - task: KubernetesManifest@0
            displayName: Apply canary for $(strategy.increment)% traffic
            inputs:
              action: deploy
              namespace: $(namespace)
              manifests: $(Pipeline.Workspace)/canary.yml
              imagePullSecrets: $(imageName)

        # postRouteTraffic:
        #   # postRouteTraffic - Use to run the tasks after the traffic is routed. Typically these tasks
        #   # monitor the health of the updated version for defined interval. The results of a lifecycle
        #   # hook event can trigger a rollback.
        #   steps:
        #   - checkout: self
        #   - script: ./deploy/perform-e2e-tests.sh

        on:
          # if the canary has failed,
          # 1) remove the Mapping that sends traffic to the canary, as well as the canary Service and Deployment
          failure:
            steps:
            # remove all the Service/Deployment/Mappings of the canary
            - task: KubernetesManifest@0
              displayName: Remove the Canary
              inputs:
                action: delete
                arguments: services,deployments,mappings -l canary=true,build=$(Build.BuildNumber)
                namespace: $(namespace)

          # if the canary has been successful,
          # 1) re-apply the manifests but with the image set to use $(buildTag)
          # 2) remove the Mapping that sends traffic to the canary, as well as the canary Service and Deployment
          success:
            steps:
            # upgrade the image in the base Service/Deployment for using tag=$(buildTag)
            # note: we could also use a "Patch" action (https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/deploy/kubernetes-manifest?view=azure-devops#patch-action)
            #       or a "kubectl set image" (https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment)
            - checkout: self
            - task: KubernetesManifest@0
              displayName: Set $(buildTag) as image in base Deployment
              inputs:
                action: deploy
                namespace: $(namespace)
                manifests: $(Build.SourcesDirectory)/manifests/*.yml
                # for every image $(containerRegistry).azurecr.io/$(imageName):*, replace the tag by $(buildTag)
                containers: $(containerRegistry).azurecr.io/$(imageName):$(buildTag)
                imagePullSecrets: $(imageName)

            # remove all the Service/Deployment/Mappings of the canary
            - task: KubernetesManifest@0
              displayName: Remove the Canary
              inputs:
                action: delete
                arguments: services,deployments,mappings -l canary=true,build=$(Build.BuildNumber)
                namespace: $(namespace)
